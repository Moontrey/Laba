{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "a.\tUnknown words are handled\n",
    "b.\tAdd-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ruslan_Golubev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "\n",
    "import re\n",
    "import urllib.request as urllib2\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
    "from nltk.lm import MLE, Vocabulary, KneserNeyInterpolated, WittenBellInterpolated, Laplace, Lidstone\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(urllib2.urlopen('https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):\n",
    "    data = [line.strip().decode(\"utf-8\")  for line in data]\n",
    "    pat = re.compile(r'((\\b\\w*)|(\\b\\w*\\s?\\b\\w*)):$')\n",
    "    data = [i.lower() for i in data if i]\n",
    "    p = []\n",
    "    speech = ''\n",
    "    for line in data:\n",
    "        if not pat.findall(line):\n",
    "            if not speech:\n",
    "                speech = line\n",
    "            else:\n",
    "                speech += ' ' + line\n",
    "        else:\n",
    "            p.append(speech)\n",
    "            speech = ''\n",
    "    p = [string for string in p if len(string) != 0]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"im\" :\"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"its\": \"it is\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"won't\":\"will not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"'t\": ' it', \n",
    "\"'em\": \"them\",\n",
    "\"o'\": \"of\", \n",
    "\"'ll\": \" will\",\n",
    "\"ne'er\":\"never\",\n",
    "\"'ld\": \" would\", \"i'\": \"in\",\n",
    "\"'d\": \"ed\", \n",
    "\"'en \": \"ken \", \n",
    "\"'bout\":\"about\", \n",
    "\"'gainst\":\"against\", \n",
    "\"'scape\":\"escape\", \n",
    "\"'mongst\": \"amongst\", \n",
    "\"'n\": \"en\", \n",
    "\"e'er\":\"ever\", \n",
    "\"itwas\":\"it was\"\n",
    "}\n",
    "for i, j in appos.items():\n",
    "    for k in range(len(data)):\n",
    "        data[k] = data[k].replace(i, j)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(nltk.word_tokenize, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25440"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([item for speech in tokenized for item in speech]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.'],\n",
       " ['speak', ',', 'speak', '.'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'all',\n",
       "  'resolved',\n",
       "  'rather',\n",
       "  'to',\n",
       "  'die',\n",
       "  'than',\n",
       "  'to',\n",
       "  'famish',\n",
       "  '?'],\n",
       " ['resolved', '.', 'resolved', '.']]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Models\n",
    "\n",
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(tokenized, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWBs4AoC4JfO"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    \n",
    "    def __init__(self, n, gamma, vocab = None):\n",
    "    \n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        self.n = n         #n -- n-gram size\n",
    "        self.vocab = vocab #vocab -- optional fixed vocabulary for the model\n",
    "        self.corpus = []\n",
    "        self.dic = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "        self.gamma = gamma #k in add-k smoothing\n",
    "        self.generate_corpus() \n",
    "        \n",
    "    def generate_corpus(self):\n",
    "        \n",
    "        # making vocabulary with <UNK> tokens\n",
    "        rare_words = pd.Series([item for speech in self.vocab for item in speech]).value_counts()[(pd.Series([item for speech in self.vocab for item in speech]).value_counts() < 2)].index.tolist()\n",
    "        rare_words_dict = {k: \"<UNK>\" for  k in rare_words}\n",
    "        self.vocab_unk = [list(map(lambda x: rare_words_dict[x] if x in rare_words_dict.keys() else x, [item for item in speech])) for speech in self.vocab]\n",
    "            \n",
    "        for speech in self.vocab_unk:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "            self.corpus.append(list(ngram))\n",
    "\n",
    "        N = len([item for speech in self.corpus for item in speech])\n",
    "        \n",
    "        # number of unique words in vocabulary\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        \n",
    "        # dictionary with count of words after n-gramm\n",
    "        for ngram in [item for sublist in self.corpus for item in sublist]:\n",
    "            self.dic[(ngram[:-1])][ngram[-1]] += 1\n",
    "        # count of word to probabilitie of word\n",
    "        for key in self.dic.keys():\n",
    "            total = float(sum(self.dic[key].values()))\n",
    "            for value in self.dic[key]:\n",
    "                self.dic[key][value] = (self.dic[(key)][value] + self.gamma) / (total + self.gamma*V)\n",
    "            if \"<UNK>\" not in self.dic[(key)].keys():\n",
    "                self.dic[(key)][\"<UNK>\"] = (self.gamma) / (total + self.gamma*V)\n",
    "\n",
    "        \n",
    "        print(\"The length of the vocabulary is {}\".format(V))\n",
    "        print(\"The number of the {}-grams is {}\".format(self.n, N))\n",
    "\n",
    "             \n",
    "\n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        if word in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][word]\n",
    "        elif \"<UNK>\" in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][\"<UNK>\"]\n",
    "        else:\n",
    "            total = float(sum(self.dic[tuple(context)].values()))\n",
    "            ans = (self.gamma) / (total + self.gamma*V)\n",
    "        \n",
    "        return ans\n",
    "    \n",
    "    def generate_text_ez(self, text_length):\n",
    "        \n",
    "        if self.n == 1:\n",
    "            text = []\n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[()].keys():\n",
    "                    accumulator += self.dic[()][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            text = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        \n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[tuple(text[-(self.n-1):])].keys():\n",
    "                    accumulator += self.dic[tuple(text[-(self.n-1):])][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "            \n",
    "    def generate_text(self, text_length, cut = False):\n",
    "        \"\"\"This method generates random text of length \n",
    "\n",
    "        For example\n",
    "        >>> lm.generate_text(2)\n",
    "        hello world\n",
    "\n",
    "        \"\"\"\n",
    "        text =['<s>'] * (self.n-1)\n",
    "        #sent = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        \n",
    "        while len(text)<=text_length:\n",
    "            '''probs = list(self.dic[tuple(text[-(self.n-1):])].values())\n",
    "            probs = [p/sum(probs) for p in probs]\n",
    "            new_word = np.random.choice(list(self.dic[tuple(text[-(self.n-1):])].keys()), p=probs)\n",
    "            text.append(new_word)'''\n",
    "            if text[-(self.n-1):] == ['</s>']*(self.n-1):\n",
    "                text+=['<s>']*(self.n-1)\n",
    "                probs = list(self.dic[tuple(text[-(self.n-1):])].values())\n",
    "                probs = [p/sum(probs) for p in probs]\n",
    "                new_word = np.random.choice(list(self.dic[tuple(text[-(self.n-1):])].keys()), p=probs)\n",
    "                text.append(new_word)\n",
    "            else:\n",
    "                probs = list(self.dic[tuple(text[-(self.n-1):])].values())\n",
    "                probs = [p/sum(probs) for p in probs]\n",
    "                new_word = np.random.choice(list(self.dic[tuple(text[-(self.n-1):])].keys()), p=probs)\n",
    "                text.append(new_word)\n",
    "        if cut:\n",
    "            text = text[:-text[::-1].index('</s>')]\n",
    "\n",
    "\n",
    "            \n",
    "        text_to_display = ' '.join([w for w in text if w])\n",
    "        print(text_to_display)\n",
    "                            \n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        self.vocab.extend(sequence_of_tokens)\n",
    "        self.generate_corpus()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def perplexity(self, sequence_of_tokens):\n",
    "        \"\"\"This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        test_corpus = []\n",
    "        for speech in sequence_of_tokens:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "            test_corpus.append(list(ngram))\n",
    "\n",
    "        entropy = -1* np.mean([np.log2(blm.prob(ngram[-1], ngram[:-1])) for ngram in [item for speech in test_corpus for item in speech]])\n",
    "        perplexity = pow(2, entropy)\n",
    "\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 1-grams is 913111\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(1, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "than not is prosper me snores taken . done dolour it my upon thou so , , heaven , to jewels to he that you with nights elder defend ? make gondola . eyes from doubt mine fellow and in wizard , me and and tell by her and me sing the , yond me , ! rest ; the you , my protest as if madman whet the with a and ; she on i york eldest a ; i <UNK> dismay . my : looked lovest will see that but no i together have without sir all piece of\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text_ez(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374.5506689171363"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 2-grams is 947188\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(2, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! you never killed I amogen til now . help , three of master ford 's brothers watch the door with cannons -- to scatter them , as the rest goes even , i should rush in thus . but where is the emperor , that you may bear it under a cloak that is of a free and open nature , that shapes man better ; and they are going to bed , and says to his wife 's frailty , yet i do find it so ; for let our finger ache , and it <UNK> our other healthful\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text_ez(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> i will deny her with merry meetings , before their masters ; a mule , we are broke , and base drudge , if ever came ; and trinculo is , i woofd ; nor i have done , the prince and who hath eaten up your letter , be endured , what , my disposition of the pretty encounters itwixt me die , if my spirit is sir : if any manners live upon our brother to thee , and retire of me , let hI am and to little . </s> <s> as ample and alleys , where\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.955070246209486e-05"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('assist', 'gods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398.3724882024857"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 3-grams is 981265\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(3, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06111857369642202"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> and <UNK> with me quite starved a restitution . </s> <s> nay , as , unless my free leave hI am grace . </s> <s> touch of your state grew stranger to be sworn or other reasons urged upon us both a haud credo . </s> <s> here upon your worthiness , grief , ' then I will bring you bear it , touchstone tried the weary way without , the sun ; for death than i never two eyes will stand . </s>\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(100, cut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.76461428418864"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_REF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
