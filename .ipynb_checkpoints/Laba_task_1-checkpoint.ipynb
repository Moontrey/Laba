{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "a.\tUnknown words are handled\n",
    "b.\tAdd-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "\n",
    "import re\n",
    "import urllib.request as urllib2\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
    "from nltk.lm import MLE, Vocabulary, KneserNeyInterpolated, WittenBellInterpolated, Laplace, Lidstone\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(urllib2.urlopen('https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):\n",
    "    data = [line.strip().decode(\"utf-8\")  for line in data]\n",
    "    pat = re.compile(r'((\\b\\w*)|(\\b\\w*\\s?\\b\\w*)):$')\n",
    "    data = [i.lower() for i in data if i]\n",
    "    p = []\n",
    "    speech = ''\n",
    "    for line in data:\n",
    "        if not pat.findall(line):\n",
    "            if not speech:\n",
    "                speech = line\n",
    "            else:\n",
    "                speech += ' ' + line\n",
    "\n",
    "        else:\n",
    "            p.append(speech)\n",
    "            speech = ''\n",
    "    p = [string for string in p if len(string) != 0]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"im\" :\"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"its\": \"it is\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"won't\":\"will not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"'t\": ' it', \n",
    "\"'em\": \"them\",\n",
    "\"o'\": \"of\", \n",
    "\"'ll\": \" will\",\n",
    "\"ne'er\":\"never\",\n",
    "\"'ld\": \" would\", \"i'\": \"in\",\n",
    "\"'d\": \"ed\", \n",
    "\"'en \": \"ken \", \n",
    "\"'bout\":\"about\", \n",
    "\"'gainst\":\"against\", \n",
    "\"'scape\":\"escape\", \n",
    "\"'mongst\": \"amongst\", \n",
    "\"'n\": \"en\", \n",
    "\"e'er\":\"ever\", \n",
    "\"itwas\":\"it was\"\n",
    "}\n",
    "for i, j in appos.items():\n",
    "    for k in range(len(data)):\n",
    "        data[k] = data[k].replace(i, j)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(nltk.word_tokenize, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25440"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([item for speech in tokenized for item in speech]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.'],\n",
       " ['speak', ',', 'speak', '.'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'all',\n",
       "  'resolved',\n",
       "  'rather',\n",
       "  'to',\n",
       "  'die',\n",
       "  'than',\n",
       "  'to',\n",
       "  'famish',\n",
       "  '?'],\n",
       " ['resolved', '.', 'resolved', '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Models\n",
    "\n",
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(tokenized, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWBs4AoC4JfO"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    \n",
    "    def __init__(self, n, gamma, vocab = None):\n",
    "    \n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        self.n = n         #n -- n-gram size\n",
    "        self.vocab = vocab #vocab -- optional fixed vocabulary for the model\n",
    "        self.corpus = []\n",
    "        self.dic = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "        self.gamma = gamma #k in add-k smoothing\n",
    "        self.generate_corpus() \n",
    "        \n",
    "    def generate_corpus(self):\n",
    "        \n",
    "        # making vocabulary with <UNK> tokens\n",
    "        rare_words = pd.Series([item for speech in self.vocab for item in speech]).value_counts()[(pd.Series([item for speech in self.vocab for item in speech]).value_counts() < 2)].index.tolist()\n",
    "        rare_words_dict = {k: \"<UNK>\" for  k in rare_words}\n",
    "        self.vocab_unk = [list(map(lambda x: rare_words_dict[x] if x in rare_words_dict.keys() else x, [item for item in speech])) for speech in self.vocab]\n",
    "            \n",
    "        for speech in self.vocab_unk:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            self.corpus.append(list(ngram))\n",
    "\n",
    "        N = len([item for speech in self.corpus for item in speech])\n",
    "        \n",
    "        # number of unique words in vocabulary\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        \n",
    "        # dictionary with count of words after n-gramm\n",
    "        for ngram in [item for sublist in self.corpus for item in sublist]:\n",
    "            self.dic[(ngram[:-1])][ngram[-1]] += 1\n",
    "        # count of word to probabilitie of word\n",
    "        for key in self.dic.keys():\n",
    "            total = float(sum(self.dic[key].values()))\n",
    "            for value in self.dic[key]:\n",
    "                self.dic[key][value] = (self.dic[(key)][value] + self.gamma) / (total + self.gamma*V)\n",
    "\n",
    "\n",
    "        print(\"The length of the vocabulary is {}\".format(V))\n",
    "        print(\"The number of the {}-grams is {}\".format(self.n, N))\n",
    "\n",
    "             \n",
    "\n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        if word in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][word]\n",
    "        elif \"<UNK>\" in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][\"<UNK>\"]\n",
    "        else:\n",
    "            total = float(sum(self.dic[tuple(context)].values()))\n",
    "            ans = (self.gamma) / (total + self.gamma*V)\n",
    "        \n",
    "        return ans\n",
    "    \n",
    "    def generate_text_ez(self, text_length):\n",
    "        \n",
    "        if self.n == 1:\n",
    "            text = []\n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[()].keys():\n",
    "                    accumulator += self.dic[()][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            text = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        \n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[tuple(text[-(self.n-1):])].keys():\n",
    "                    accumulator += self.dic[tuple(text[-(self.n-1):])][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "            \n",
    "\n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        self.vocab.extend(sequence_of_tokens)\n",
    "        self.generate_corpus()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def perplexity(self, sequence_of_tokens):\n",
    "        \"\"\"This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        test_corpus = []\n",
    "        for speech in sequence_of_tokens:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            test_corpus.append(list(ngram))\n",
    "\n",
    "        entropy = -1* np.mean([np.log2(blm.prob(ngram[-1], ngram[:-1])) for ngram in [item for speech in test_corpus for item in speech]])\n",
    "        perplexity = pow(2, entropy)\n",
    "\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 3-grams is 981265\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(3, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these thou my lay to I , out . , god ? . ! this state retire from again weariness matter brain in lay necessity yourself for am by again day , i me petruchio i it peace i , her sold worth is fare all , or follow as power you truly buckled . our honour . much <UNK> is and anatomy fair , though my his for them thee then ; ; , o princes be i left reproaches is and your duke sisterhood an ! death wish , , hI 'it ! i , plot are a near\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text_ez(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06111857369642202"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14427\n",
      "The number of the 3-grams is 981814\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(3, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060936263468679606"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prospero be living and die. i \n",
      " am not; for, i am a gentleman \n",
      " of great worth and honour. i \n",
      " will not be so, i will not be \n",
      " so, my lord, i will not be \n",
      " so, my lord, i will not be \n",
      " excused. i will not be a \n",
      " <UNK>, and i will not be uplifted. \n",
      " but, i am not; but, as \n",
      " i am a gentleman? i am \n",
      " not; stand aside. i will \n",
      " not be a <UNK>, and the <UNK> of \n",
      " the world, with his own <UNK>. \n",
      " i am not, sir, i will \n",
      " not be a man, i am a gentleman \n",
      " of blood and death will have a daughter called \n",
      " katharina. i am not; i \n",
      " will not be a <UNK>, and i will \n",
      " not be a <UNK>, and i will not \n",
      " be so, i am a gentleman. come \n",
      ", come, come, come, come, \n",
      " come, come, come, come, come \n",
      ", come, come they to have redress against \n",
      " them, and the <UNK> of the world, \n",
      " i will not be so, my lord, \n",
      " i am a man of <UNK>, and i \n",
      " will not be so, sir, i will \n",
      " not be a <UNK>, and i will not \n",
      " be a <UNK> rudesby full of <UNK>, and \n",
      " the <UNK> of the world, in the world \n",
      ", he is a good man's life, \n",
      " and i will not be a <UNK>, and \n",
      " i will not be so, my lord, \n",
      " i will not be a <UNK>, and the \n",
      " <UNK> of the world, to be a <UNK> \n",
      ", and i will not be so, my \n",
      " lord, i will not endure it now, \n",
      " my lord, i am a gentleman of the \n",
      " world, to be a <UNK>, and i \n",
      " will not be a <UNK>, and i will \n",
      " not be a <UNK>, and i will not \n",
      " be so, my lord, i am not \n",
      "; but, i will not be so, \n",
      " my lord, i am a gentleman my dear \n",
      " lord edward! i am not; \n",
      " i will not be a <UNK>, and i \n",
      " will not be a <UNK>, and the <UNK> \n",
      " of the world, that i have a care \n",
      " this busy tI ame, and the <UNK> of \n",
      " the world, i am a gentleman. \n",
      " i am not to be a <UNK>, \n",
      " and i will not be so, i am \n",
      " a gentleman of the world, not a word \n",
      ". i am a gentleman you send \n",
      " for lucius his son, and i will not \n",
      " be so, my lord,\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331.5135812006942"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14427\n",
      "The number of the 4-grams is 1015891\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(4, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0648862384131717"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('the', 'gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1832.0852398798609"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting words\n",
    "text = ['before',  'we']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before we met , mistress , <UNK> . ay\n"
     ]
    }
   ],
   "source": [
    "while len(text)<=10:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "\n",
    "    for word in blm.dic[tuple(text[-2:])].keys():\n",
    "        accumulator += blm.dic[tuple(text[-2:])][word]\n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "        \n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down', '<UNK>']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list(list(blm.dic.keys())[random.randint(0, len(blm.dic))])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down <UNK> : the god and the firm roman to great and trusty business in to me . the sword it fights with . where is the matter of heavy mind i sway by and by thy flight lay toward the grecian dames are sunburnt and not his . right . good sir , i have already order this night . know that i could have turned you to let an old man , old jack ; die when i kissed the <UNK> and take this along ; and -- return the lie another tI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while len(text)<=100:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "\n",
    "    for word in blm.dic[tuple(text[-(n-1):])].keys():\n",
    "        accumulator += blm.dic[tuple(text[-(n-1):])][word]\n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "        \n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(blm.dic.keys())[random.randint(0, len(blm.dic))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_REF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
