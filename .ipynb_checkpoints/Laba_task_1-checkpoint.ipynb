{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "a.\tUnknown words are handled\n",
    "b.\tAdd-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "\n",
    "import re\n",
    "import urllib.request as urllib2\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
    "from nltk.lm import MLE, Vocabulary, KneserNeyInterpolated, WittenBellInterpolated, Laplace, Lidstone\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(urllib2.urlopen('https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):\n",
    "    data = [line.strip().decode(\"utf-8\")  for line in data]\n",
    "    pat = re.compile(r'((\\b\\w*)|(\\b\\w*\\s?\\b\\w*)):$')\n",
    "    data = [i.lower() for i in data if i]\n",
    "    p = []\n",
    "    speech = ''\n",
    "    for line in data:\n",
    "        if not pat.findall(line):\n",
    "            if not speech:\n",
    "                speech = line\n",
    "            else:\n",
    "                speech += ' ' + line\n",
    "\n",
    "        else:\n",
    "            p.append(speech)\n",
    "            speech = ''\n",
    "    p = [string for string in p if len(string) != 0]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"im\" :\"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"its\": \"it is\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"won't\":\"will not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"'t'\": ' it', \n",
    "\"'em\": \"them\",\n",
    "\"o'\": \"of\", \n",
    "\"'ll\": \" will\",\n",
    "\"ne'er\":\"never\",\n",
    "\"'ld\": \" would\", \"i'\": \"in\",\n",
    "\"'d\": \"ed\", \n",
    "\"'en \": \"ken \", \n",
    "\"'bout\":\"about\", \n",
    "\"'gainst\":\"against\", \n",
    "\"'scape\":\"escape\", \n",
    "\"'mongst\": \"amongst\", \n",
    "\"'n\": \"en\", \n",
    "\"e'er\":\"ever\", \n",
    "\"itwas\":\"it was\"\n",
    "}\n",
    "for i, j in appos.items():\n",
    "    for k in range(len(data)):\n",
    "        data[k] = data[k].replace(i, j)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(nltk.word_tokenize, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.'],\n",
       " ['speak', ',', 'speak', '.'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'all',\n",
       "  'resolved',\n",
       "  'rather',\n",
       "  'to',\n",
       "  'die',\n",
       "  'than',\n",
       "  'to',\n",
       "  'famish',\n",
       "  '?'],\n",
       " ['resolved', '.', 'resolved', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Models\n",
    "\n",
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(tokenized, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWBs4AoC4JfO"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    \n",
    "    def __init__(self, n, gamma, vocab = None):\n",
    "    \n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        self.corpus = []\n",
    "        self.dic = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.gamma = gamma\n",
    "        self.generate_corpus()\n",
    "        \n",
    "    def generate_corpus(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        The formula for add-k smoothing is:\n",
    "\n",
    "        p(w_i|w_i-1) = (c(w_i-1, w-i) + k) / (c(w_i-1) + k*V),\n",
    "\n",
    "        where V is the number of unique  words in the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "      \n",
    "        rare_words = pd.Series([item for speech in self.vocab for item in speech]).value_counts()[(pd.Series([item for speech in self.vocab for item in speech]).value_counts() < 2)].index.tolist()\n",
    "        rare_words_dict = {k: \"<UNK>\" for  k in rare_words}\n",
    "        self.vocab_unk = [list(map(lambda x: rare_words_dict[x] if x in rare_words_dict.keys() else x, [item for item in speech])) for speech in self.vocab]\n",
    "            \n",
    "        for speech in self.vocab_unk:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            self.corpus.append(list(ngram))\n",
    "\n",
    "        N = len([item for speech in self.corpus for item in speech])\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "            \n",
    "        for ngram in [item for sublist in self.corpus for item in sublist]:\n",
    "            self.dic[(ngram[:-1])][ngram[-1]] += 1\n",
    "\n",
    "        for key in self.dic.keys():\n",
    "            total = float(sum(self.dic[key].values()))\n",
    "            for value in self.dic[key]:\n",
    "                self.dic[key][value] = (self.dic[(key)][value] + self.gamma) / (total + self.gamma*V)\n",
    "\n",
    "\n",
    "        print(\"The length of the vocabulary is {}\".format(V))\n",
    "        print(\"The number of the {}-grams is {}\".format(self.n, N))\n",
    "\n",
    "             \n",
    "\n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        if word in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][word]\n",
    "        elif \"<UNK>\" in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][\"<UNK>\"]\n",
    "        else:\n",
    "            total = float(sum(self.dic[tuple(context)].values()))\n",
    "            ans = (self.gamma) / (total + self.gamma*V)\n",
    "        \n",
    "        return ans\n",
    " \n",
    "    def generate_text(self, text_length):\n",
    "        \"\"\"This method generates random text of length \n",
    "\n",
    "        For example\n",
    "        >>> lm.generate_text(2)\n",
    "        hello world\n",
    "\n",
    "        \"\"\"\n",
    "        text = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        while len(text)<=text_length:\n",
    "  \n",
    "            probs = scaler.fit_transform(np.array(list(self.dic[tuple(text[-(self.n-1):])].values())).reshape(-1, 1))\n",
    "            probs = softmax(probs).reshape(-1)\n",
    "            new_word = np.random.choice(list(self.dic[tuple(text[-(self.n-1):])].keys()), p=probs)\n",
    "            text.append(new_word)\n",
    "                \n",
    "        for ind, word in enumerate(text):\n",
    "            if ind % 10 == 0:\n",
    "                text.insert(ind, \"\\n\")\n",
    "            \n",
    "        text_to_display = ' '.join([w for w in text if w])\n",
    "        for mark in re.findall(r\"\\s[.;:,!?\\\\']\", text_to_display):\n",
    "            text_to_display= text_to_display.replace(mark, mark[-1:])\n",
    "        \n",
    "        print(text_to_display)\n",
    "                \n",
    "\n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        self.vocab.extend(sequence_of_tokens)\n",
    "        self.generate_corpus()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def perplexity(self, sequence_of_tokens):\n",
    "        \"\"\"This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        test_corpus = []\n",
    "        for speech in sequence_of_tokens:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            test_corpus.append(list(ngram))\n",
    "\n",
    "        entropy = -1* np.mean([np.log2(blm.prob(ngram[-1], ngram[:-1])) for ngram in [item for speech in test_corpus for item in speech]])\n",
    "        perplexity = pow(2, entropy)\n",
    "\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14427\n",
      "The number of the 2-grams is 947737\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(2, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.931447979482915e-05"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('assist', 'gods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pacified: i have you, and, and \n",
      ", and, and, and, and, \n",
      " and, and, and the <UNK>, and \n",
      ", and, and, and the <UNK>, \n",
      " and, and, and i am, and \n",
      ", and i am, and\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.60719326192819"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14427\n",
      "The number of the 3-grams is 981814\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(3, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060936263468679606"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prospero be living and die. i \n",
      " am not; for, i am a gentleman \n",
      " of great worth and honour. i \n",
      " will not be so, i will not be \n",
      " so, my lord, i will not be \n",
      " so, my lord, i will not be \n",
      " excused. i will not be a \n",
      " <UNK>, and i will not be uplifted. \n",
      " but, i am not; but, as \n",
      " i am a gentleman? i am \n",
      " not; stand aside. i will \n",
      " not be a <UNK>, and the <UNK> of \n",
      " the world, with his own <UNK>. \n",
      " i am not, sir, i will \n",
      " not be a man, i am a gentleman \n",
      " of blood and death will have a daughter called \n",
      " katharina. i am not; i \n",
      " will not be a <UNK>, and i will \n",
      " not be a <UNK>, and i will not \n",
      " be so, i am a gentleman. come \n",
      ", come, come, come, come, \n",
      " come, come, come, come, come \n",
      ", come, come they to have redress against \n",
      " them, and the <UNK> of the world, \n",
      " i will not be so, my lord, \n",
      " i am a man of <UNK>, and i \n",
      " will not be so, sir, i will \n",
      " not be a <UNK>, and i will not \n",
      " be a <UNK> rudesby full of <UNK>, and \n",
      " the <UNK> of the world, in the world \n",
      ", he is a good man's life, \n",
      " and i will not be a <UNK>, and \n",
      " i will not be so, my lord, \n",
      " i will not be a <UNK>, and the \n",
      " <UNK> of the world, to be a <UNK> \n",
      ", and i will not be so, my \n",
      " lord, i will not endure it now, \n",
      " my lord, i am a gentleman of the \n",
      " world, to be a <UNK>, and i \n",
      " will not be a <UNK>, and i will \n",
      " not be a <UNK>, and i will not \n",
      " be so, my lord, i am not \n",
      "; but, i will not be so, \n",
      " my lord, i am a gentleman my dear \n",
      " lord edward! i am not; \n",
      " i will not be a <UNK>, and i \n",
      " will not be a <UNK>, and the <UNK> \n",
      " of the world, that i have a care \n",
      " this busy tI ame, and the <UNK> of \n",
      " the world, i am a gentleman. \n",
      " i am not to be a <UNK>, \n",
      " and i will not be so, i am \n",
      " a gentleman of the world, not a word \n",
      ". i am a gentleman you send \n",
      " for lucius his son, and i will not \n",
      " be so, my lord,\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331.5135812006942"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14427\n",
      "The number of the 4-grams is 1015891\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(4, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0648862384131717"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('the', 'gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " with this thin helm? mine enemy's dog \n",
      ", and that is the way to make an \n",
      " ass of me? i am \n",
      " not what i am about, till candles and \n",
      " starlight and moonshine be out. \n",
      " i will not be long; meantI ame, \n",
      " but hearts for the event. \n",
      " i will not, hold me still; the \n",
      " tI ame, and, in the name lay \n",
      " a moiety of my estate to your ring, \n",
      " i will not be long; that the property \n",
      " of youth and bloom of lustihood. \n",
      " i am not <UNK> here? \n",
      " i will not. i \n",
      " am glad to see you. \n",
      " i am not a hilding, hold me your \n",
      " loyal servant, your physician, your most dear \n",
      " daughter -- i am for you \n",
      ". i am nothing: but \n",
      " whatever i be, nor goodly ilion stand; \n",
      " our firebrand brother, paris, burns us all \n",
      ". i am, and bring \n",
      " hI am to the tower, and was fidele \n",
      ". what think you? i \n",
      " am not mad: i would not wish a \n",
      " drop of blood in me i \n",
      " am not well in health, and that is \n",
      " myself. give me some help here, ho \n",
      "? i think you are not, which, \n",
      " when it stands well with her. ah, \n",
      " sirrah! quoth-a, we shall not marry till \n",
      " thou bid'st us. i am \n",
      " not <UNK> and rash? i \n",
      " am glad to see you are become so penitent \n",
      ". <UNK> and berkeley, go along with me \n",
      ". i have no more of \n",
      " his courtesy than your deserving. \n",
      " i am not to be found false and recreant \n",
      ", to prove hI am, and bury all \n",
      ", which yet from her by her own report \n",
      ". i am a subject, \n",
      " i am a gentleman. i \n",
      " am not merry; but yet, -- \n",
      " i am glad to have you understand \n",
      " me: over and above, in love <UNK> \n",
      ", shall i be forsworn; to love fair \n",
      " silvia, shall i be tempted of the devil \n",
      ", and my lord! i \n",
      " am glad to clasp thee. \n",
      " i am glad you have nobody here. \n",
      " i am known to be contemned, \n",
      " than still contemned and flattered. to be now \n",
      " a sensible man, by thy patient's side \n",
      "; and, he returning to break our country \n",
      "'s laws. i am one \n",
      " that had a head to hit\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1832.0852398798609"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_REF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
