{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "a.\tUnknown words are handled\n",
    "b.\tAdd-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "\n",
    "import re\n",
    "import urllib.request as urllib2\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
    "from nltk.lm import MLE, Vocabulary, KneserNeyInterpolated, WittenBellInterpolated, Laplace, Lidstone\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(urllib2.urlopen('https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):\n",
    "    data = [line.strip().decode(\"utf-8\")  for line in data]\n",
    "    pat = re.compile(r'((\\b\\w*)|(\\b\\w*\\s?\\b\\w*)):$')\n",
    "    data = [i.lower() for i in data if i]\n",
    "    p = []\n",
    "    speech = ''\n",
    "    for line in data:\n",
    "        if not pat.findall(line):\n",
    "            if not speech:\n",
    "                speech = line\n",
    "            else:\n",
    "                speech += ' ' + line\n",
    "\n",
    "        else:\n",
    "            p.append(speech)\n",
    "            speech = ''\n",
    "    p = [string for string in p if len(string) != 0]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"im\" :\"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"its\": \"it is\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"won't\":\"will not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"'t\": ' it', \n",
    "\"'em\": \"them\",\n",
    "\"o'\": \"of\", \n",
    "\"'ll\": \" will\",\n",
    "\"ne'er\":\"never\",\n",
    "\"'ld\": \" would\", \"i'\": \"in\",\n",
    "\"'d\": \"ed\", \n",
    "\"'en \": \"ken \", \n",
    "\"'bout\":\"about\", \n",
    "\"'gainst\":\"against\", \n",
    "\"'scape\":\"escape\", \n",
    "\"'mongst\": \"amongst\", \n",
    "\"'n\": \"en\", \n",
    "\"e'er\":\"ever\", \n",
    "\"itwas\":\"it was\"\n",
    "}\n",
    "for i, j in appos.items():\n",
    "    for k in range(len(data)):\n",
    "        data[k] = data[k].replace(i, j)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(nltk.word_tokenize, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25440"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([item for speech in tokenized for item in speech]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.'],\n",
       " ['speak', ',', 'speak', '.'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'all',\n",
       "  'resolved',\n",
       "  'rather',\n",
       "  'to',\n",
       "  'die',\n",
       "  'than',\n",
       "  'to',\n",
       "  'famish',\n",
       "  '?'],\n",
       " ['resolved', '.', 'resolved', '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Models\n",
    "\n",
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(tokenized, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWBs4AoC4JfO"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    \n",
    "    def __init__(self, n, gamma, vocab = None):\n",
    "    \n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        self.n = n         #n -- n-gram size\n",
    "        self.vocab = vocab #vocab -- optional fixed vocabulary for the model\n",
    "        self.corpus = []\n",
    "        self.dic = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "        self.gamma = gamma #k in add-k smoothing\n",
    "        self.generate_corpus() \n",
    "        \n",
    "    def generate_corpus(self):\n",
    "        \n",
    "        # making vocabulary with <UNK> tokens\n",
    "        rare_words = pd.Series([item for speech in self.vocab for item in speech]).value_counts()[(pd.Series([item for speech in self.vocab for item in speech]).value_counts() < 2)].index.tolist()\n",
    "        rare_words_dict = {k: \"<UNK>\" for  k in rare_words}\n",
    "        self.vocab_unk = [list(map(lambda x: rare_words_dict[x] if x in rare_words_dict.keys() else x, [item for item in speech])) for speech in self.vocab]\n",
    "            \n",
    "        for speech in self.vocab_unk:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            self.corpus.append(list(ngram))\n",
    "\n",
    "        N = len([item for speech in self.corpus for item in speech])\n",
    "        \n",
    "        # number of unique words in vocabulary\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        \n",
    "        # dictionary with count of words after n-gramm\n",
    "        for ngram in [item for sublist in self.corpus for item in sublist]:\n",
    "            self.dic[(ngram[:-1])][ngram[-1]] += 1\n",
    "        # count of word to probabilitie of word\n",
    "        for key in self.dic.keys():\n",
    "            total = float(sum(self.dic[key].values()))\n",
    "            for value in self.dic[key]:\n",
    "                self.dic[key][value] = (self.dic[(key)][value] + self.gamma) / (total + self.gamma*V)\n",
    "            if \"<UNK>\" not in self.dic[(key)].keys():\n",
    "                self.dic[(key)][\"<UNK>\"] = (self.gamma) / (total + self.gamma*V)\n",
    "        \n",
    "        print(\"The length of the vocabulary is {}\".format(V))\n",
    "        print(\"The number of the {}-grams is {}\".format(self.n, N))\n",
    "\n",
    "             \n",
    "\n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        V = len(set([item for speech in self.vocab_unk for item in speech]))\n",
    "        if word in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][word]\n",
    "        elif \"<UNK>\" in self.dic[tuple(context)].keys():\n",
    "            ans = self.dic[tuple(context)][\"<UNK>\"]\n",
    "        else:\n",
    "            total = float(sum(self.dic[tuple(context)].values()))\n",
    "            ans = (self.gamma) / (total + self.gamma*V)\n",
    "        \n",
    "        return ans\n",
    "    \n",
    "    def generate_text_ez(self, text_length):\n",
    "        \n",
    "        if self.n == 1:\n",
    "            text = []\n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[()].keys():\n",
    "                    accumulator += self.dic[()][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            text = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        \n",
    "            while len(text) <= text_length:\n",
    "                # select a random probability threshold  \n",
    "                r = random.random()\n",
    "                accumulator = .0\n",
    "\n",
    "                for word in self.dic[tuple(text[-(self.n-1):])].keys():\n",
    "                    accumulator += self.dic[tuple(text[-(self.n-1):])][word]\n",
    "                    # select words that are above the probability threshold\n",
    "                    if accumulator >= r:\n",
    "                        text.append(word)\n",
    "                        break\n",
    "        \n",
    "            print(' '.join([t for t in text if t]))\n",
    "            \n",
    "    def generate_text(self, text_length):\n",
    "        \"\"\"This method generates random text of length \n",
    "\n",
    "        For example\n",
    "        >>> lm.generate_text(2)\n",
    "        hello world\n",
    "\n",
    "        \"\"\"\n",
    "        text = list(list(self.dic.keys())[random.randint(0, len(self.dic))])\n",
    "        \n",
    "        while len(text)<=text_length:\n",
    "  \n",
    "            probs = list(self.dic[tuple(text[-(self.n-1):])].values())\n",
    "            probs = [p/sum(probs) for p in probs]\n",
    "            new_word = np.random.choice(list(self.dic[tuple(text[-(self.n-1):])].keys()), p=probs)\n",
    "            text.append(new_word)\n",
    "            \n",
    "        text_to_display = ' '.join([w for w in text if w])\n",
    "        \n",
    "        print(text_to_display)\n",
    "                            \n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        self.vocab.extend(sequence_of_tokens)\n",
    "        self.generate_corpus()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def perplexity(self, sequence_of_tokens):\n",
    "        \"\"\"This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        test_corpus = []\n",
    "        for speech in sequence_of_tokens:\n",
    "\n",
    "            ngram = nltk.ngrams([word for word in speech], self.n, pad_right=True, pad_left=True)\n",
    "            test_corpus.append(list(ngram))\n",
    "\n",
    "        entropy = -1* np.mean([np.log2(blm.prob(ngram[-1], ngram[:-1])) for ngram in [item for speech in test_corpus for item in speech]])\n",
    "        perplexity = pow(2, entropy)\n",
    "\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 2-grams is 947188\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(2, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traps to storm of <UNK> me : you to see your hands of those which makes each other in the wars without any terms of that would have waft them weep for my liege . my meaning towards hI am in what hath heard , thou in tI ame . by venus that we go you , bear it for having bravely thou livest ! how came hither , that mingle with marvel he looks among ladies can sever themselves from all . think'st thou mark you , be plain ? if thou strike ? ' say\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text_ez(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locked up ; whose settled age and disturb devotion which frailty and practise . o most kind , and , free and shut up in the capitol . i should open . i did not ships , or cease thy poor gloucester ? will be waking . talk of . what is . I will teach thee . it ; i can carry that hast built you he is the world but in it by your ear in exclaI ams how long have no heads of virtue and kiss his will not call us , your clock\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.955070246209486e-05"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('assist', 'gods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.76461428418864"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 3-grams is 981265\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(3, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06111857369642202"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where bells have knolled to church in a <UNK> rush at once both the degrees prevent my curses . make me chide thee , for this had not churchmen prayed , that thou art here but erewhile , that , each straw , my brother clarence to your protection i commend you to give me your hand ; and point at your discords too have knit again , and turn two mincing steps into a glass . now does he so young a man , a green-a box : do not care for hI am ?\n"
     ]
    }
   ],
   "source": [
    "blm.generate_text(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398.3724882024857"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is 14378\n",
      "The number of the 4-grams is 1015342\n"
     ]
    }
   ],
   "source": [
    "blm = BaseLM(4, 0.001, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06509298998569384"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.prob('you', ('the', 'gods', 'assist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1933.316990927412"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.perplexity(X_test[:10])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_REF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
